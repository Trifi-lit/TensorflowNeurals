import tensorflow as tf
print("TensorFlow version:", tf.__version__)
from tensorflow import keras
from keras import backend as Ker
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Dense
from keras.models import Sequential
from keras.initializers import Initializer, Constant
from sklearn.cluster import KMeans
from keras.datasets import boston_housing

#initialize the centers with Kmeans algorithm
class KMeansCenterInitialize(Initializer):
    def __init__(self, X):
        self.X = X  #training data
        super().__init__()

    def __call__(self, shape, dtype=None, *args): 
        assert shape[1] == self.X.shape[1] #number of data columns(characteristics)
        n_centers = shape[0] #number of rows (observations)
        kmeans = KMeans(n_clusters=n_centers, max_iter=50) #kmeans converges at 20-50 iterations in practical situations 
        kmeans.fit(self.X)
        return kmeans.cluster_centers_
    




class RBFLayer(layers.Layer):
    def __init__(self, output_dim, betas=1.0, initializer=None, **kwargs):
        self.output_dim = output_dim
        self.initializer = initializer
        super(RBFLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.centers = self.add_weight(name='centers',
                                       shape=(self.output_dim, input_shape[1]),
                                       initializer=self.initializer,
                                       trainable=True)
        self.betas = self.add_weight(name='betas',
                                     shape=(self.output_dim,),
                                     initializer=Constant(value=1.0),trainable=True)
        super(RBFLayer, self).build(input_shape)

    def call(self, x):
        C = Ker.expand_dims(self.centers)
        H = Ker.transpose(C-Ker.transpose(x))
        return Ker.exp(-self.betas * Ker.sum(H**2, axis=1))

  
    
(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.25)
#the 13 attributes are stored in x and the corresponding prices are stored in y
#print(x_train.shape);
#print(y_train.shape);

x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)
x_test = (x_test - np.mean(x_test, axis=0)) / np.std(x_test, axis=0)


split_size = int(x_train.shape[0]*0.8)
x_train, x_val = x_train[:split_size], x_train[split_size:]
y_train, y_val = y_train[:split_size], y_train[split_size:]



def Rsquare(y_true, y_pred):
    SSR = Ker.sum(Ker.square(y_true - y_pred))
    SST = Ker.sum(Ker.square(y_true - Ker.mean(y_true)))
    return 1-SSR/(SST+ Ker.epsilon())

m = tf.keras.metrics.RootMeanSquaredError()
#Rsquare = tfa.metrics.RSquare(dtype=tf.float32, y_shape=(1,))


NeuronAmount = [round(0.1*len(x_train)), round(0.5*len(x_train)), round(0.9*len(x_train))]
percentage = ["10%", "50%", "90%"]
i=0
results = [] 

for neurons in NeuronAmount:
    i=i+1
    RBF=RBFLayer(int(neurons), initializer=KMeansCenterInitialize(x_train), input_shape=(13, ))
    
    model = Sequential()
    model.add(RBF)
    model.add(Dense(128))
    model.add(Dense(1))

    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), loss='mse', metrics=[Rsquare, m])
    history = model.fit(x_train, y_train,batch_size=32, epochs=100, validation_data=(x_val, y_val))
    #use from_logits=True because the outputs generated by the model are not normalized(no softmax layer)
    
    plt.figure(3*i)
    plt.plot(history.history['loss'], label='train')
    plt.plot(history.history['val_loss'], label='validation')
    plt.title('Learning Curve for neurons='+str(percentage[i-1])+' of training dataset')
    plt.xlabel('Epochs')
    plt.ylabel('Loss(Categorical Cross Entropy)')
    plt.legend()
    
    plt.figure(3*i+1)
    plt.plot(history.history['Rsquare'], label='train')
    plt.plot(history.history['val_Rsquare'], label='validation')
    plt.title('Rsquare for neurons='+str(percentage[i-1])+' of training dataset')
    plt.xlabel('Epochs')
    plt.ylabel('Rsquare')
    plt.legend()

    plt.figure(3*i+2)
    plt.plot(history.history['root_mean_squared_error'], label='train')
    plt.plot(history.history['val_root_mean_squared_error'], label='validation')
    plt.title('RMSE for neurons=' + str(percentage[i-1]) + ' of training dataset')
    plt.xlabel('Epochs')
    plt.ylabel('RMSE')
    plt.legend()
    results.append(model.evaluate(x_test, y_test))



print('Loss for the first neuron amount: '+str(results[0][0]))
print('Rsquare for the first neuron amount: ' + str(results[0][1]))
print('RMSE for the first neuron amount: ' + str(results[0][2]))
print('Loss for the second neuron amount: '+str(results[1][0]))
print('Rsquare for the second neuron amount: '+str(results[1][1]))
print('RMSE for the second neuron amount: ' + str(results[1][2]))
print('Loss for the third neuron amount: '+str(results[2][0]))
print('Rsquare for the third neuron amount: '+str(results[2][1]))
print('RMSE for the third neuron amount: ' + str(results[2][2]))